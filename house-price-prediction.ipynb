{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Predicting student exam scores. With Student Performance dataset on kaggle. We will EDA, then split the data and apply machine learning model (Linear Regression, Decision Tree, Random Forest, XGBoost, SVM) to comment which model is best for the data\n","metadata":{}},{"cell_type":"code","source":"#Import Libaries\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport time\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:15.919911Z","iopub.execute_input":"2025-10-24T16:12:15.920274Z","iopub.status.idle":"2025-10-24T16:12:15.926853Z","shell.execute_reply.started":"2025-10-24T16:12:15.920251Z","shell.execute_reply":"2025-10-24T16:12:15.925499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Read data description and summary\n\n","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\", \"r\") as f:\n    content = f.read()\nprint(content)  ","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-10-24T16:12:15.934724Z","iopub.execute_input":"2025-10-24T16:12:15.935076Z","iopub.status.idle":"2025-10-24T16:12:15.965194Z","shell.execute_reply.started":"2025-10-24T16:12:15.935051Z","shell.execute_reply":"2025-10-24T16:12:15.963892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. load dataset and Explore data\n\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\nsample_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:15.966945Z","iopub.execute_input":"2025-10-24T16:12:15.967330Z","iopub.status.idle":"2025-10-24T16:12:16.026130Z","shell.execute_reply.started":"2025-10-24T16:12:15.967295Z","shell.execute_reply":"2025-10-24T16:12:16.024797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assert 'Id' in test_df.columns, \"‚ùå test_df no column 'Id'\"\nassert 'Id' in sample_df.columns, \"‚ùå sample_df no column 'Id'\"\n\ntest_full_df = test_df.merge(sample_df[['Id', 'SalePrice']], on='Id', how='left')\n\nprint(f\"shape after concat: {test_full_df.shape}\")\n\ntest_full_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.027765Z","iopub.execute_input":"2025-10-24T16:12:16.028049Z","iopub.status.idle":"2025-10-24T16:12:16.082469Z","shell.execute_reply.started":"2025-10-24T16:12:16.028030Z","shell.execute_reply":"2025-10-24T16:12:16.081540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Shape dataset\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.083473Z","iopub.execute_input":"2025-10-24T16:12:16.083830Z","iopub.status.idle":"2025-10-24T16:12:16.090472Z","shell.execute_reply.started":"2025-10-24T16:12:16.083802Z","shell.execute_reply":"2025-10-24T16:12:16.089144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ntrain_df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.092673Z","iopub.execute_input":"2025-10-24T16:12:16.093220Z","iopub.status.idle":"2025-10-24T16:12:16.147069Z","shell.execute_reply.started":"2025-10-24T16:12:16.093194Z","shell.execute_reply":"2025-10-24T16:12:16.145892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.148181Z","iopub.execute_input":"2025-10-24T16:12:16.148547Z","iopub.status.idle":"2025-10-24T16:12:16.168132Z","shell.execute_reply.started":"2025-10-24T16:12:16.148516Z","shell.execute_reply":"2025-10-24T16:12:16.166858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.169256Z","iopub.execute_input":"2025-10-24T16:12:16.169744Z","iopub.status.idle":"2025-10-24T16:12:16.203872Z","shell.execute_reply.started":"2025-10-24T16:12:16.169708Z","shell.execute_reply":"2025-10-24T16:12:16.202843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Id is varible identifier because convert to object\n","metadata":{}},{"cell_type":"code","source":"train_df['Id'] = train_df['Id'].astype('object')\ntest_df['Id'] = test_df['Id'].astype('object')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.205541Z","iopub.execute_input":"2025-10-24T16:12:16.206165Z","iopub.status.idle":"2025-10-24T16:12:16.226823Z","shell.execute_reply.started":"2025-10-24T16:12:16.206133Z","shell.execute_reply":"2025-10-24T16:12:16.225748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MSSubCLass In terms of statistical significance, it is a categorical variable.","metadata":{}},{"cell_type":"code","source":"train_df['MSSubClass'] = train_df['MSSubClass'].astype('object')\ntest_df['MSSubClass'] = test_df['MSSubClass'].astype('object')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.227882Z","iopub.execute_input":"2025-10-24T16:12:16.228174Z","iopub.status.idle":"2025-10-24T16:12:16.254218Z","shell.execute_reply.started":"2025-10-24T16:12:16.228145Z","shell.execute_reply":"2025-10-24T16:12:16.253218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### handle missing values","metadata":{}},{"cell_type":"code","source":"def safe_imputer_sync(df_to_fit: pd.DataFrame, df_to_transform: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Synchronously imputes missing data in df_to_transform (TEST) using imputation parameters \n    (Median/Mode) learned from df_to_fit (TRAIN).\n\n    Args:\n        df_to_fit (pd.DataFrame): The DataFrame used to learn Median/Mode values (TRAIN).\n        df_to_transform (pd.DataFrame): The DataFrame that needs imputation (TEST).\n    \n    Returns:\n        pd.DataFrame: The DataFrame after imputation.\n    \"\"\"\n    \n    df_transformed = df_to_transform.copy()\n    \n    # 1. HANDLE NUMERIC COLUMNS - USE MEDIAN\n    # Learn numeric columns from the fit set\n    numeric_cols = df_to_fit.select_dtypes(include=np.number).columns\n    \n    for col in numeric_cols:\n        # Check if the column exists in the transformed set and has missing values\n        if col in df_transformed.columns and df_transformed[col].isnull().any():\n            # Learn the parameter (Median) ONLY from the FIT set\n            median_value = df_to_fit[col].median()\n            \n            # Apply imputation to the TRANSFORM set\n            df_transformed[col] = df_transformed[col].fillna(median_value)\n            print(f\"‚úÖ Filled '{col}' with Median from TRAIN ({median_value:.2f}).\")\n        elif col not in df_transformed.columns:\n             # Handle case where a numerical column is missing in the test set (rare after alignment)\n             pass\n\n\n    # 2. HANDLE CATEGORICAL COLUMNS - USE \"NONE\"\n    # Learn categorical columns from the fit set\n    categorical_cols = df_to_fit.select_dtypes(include=['object', 'category']).columns\n    \n    for col in categorical_cols:\n        if col in df_transformed.columns and df_transformed[col].isnull().any():\n            # Apply imputation with the constant \"None\"\n            df_transformed[col] = df_transformed[col].fillna(\"None\")\n            print(f\"‚úÖ Filled '{col}' with 'None'.\")\n            \n    print(\"\\n--- SYNCHRONIZED IMPUTATION SUMMARY ---\")\n    print(\"‚úÖ Missing Data filled successfully.\")\n    print(f\"Total Missing Values remaining: {df_transformed.isnull().sum().sum()}\")\n    return df_transformed\n\n# --- CORRECT AND SAFE USAGE ---\n\n# Step 1: Process the TRAIN set (Learning parameters from itself)\ntrain_df_imputed = safe_imputer_sync(df_to_fit=train_df, df_to_transform=train_df)\n\n# Step 2: Process the TEST set (Learning parameters from the processed TRAIN set)\ntest_df_imputed = safe_imputer_sync(df_to_fit=train_df_imputed, df_to_transform=test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.258008Z","iopub.execute_input":"2025-10-24T16:12:16.258414Z","iopub.status.idle":"2025-10-24T16:12:16.345798Z","shell.execute_reply.started":"2025-10-24T16:12:16.258383Z","shell.execute_reply":"2025-10-24T16:12:16.344533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### House Price Distribution\n##### Now let us take a look at how the house prices are distributed.","metadata":{}},{"cell_type":"code","source":"print(train_df['SalePrice'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(train_df['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.346808Z","iopub.execute_input":"2025-10-24T16:12:16.347131Z","iopub.status.idle":"2025-10-24T16:12:16.739174Z","shell.execute_reply.started":"2025-10-24T16:12:16.347103Z","shell.execute_reply":"2025-10-24T16:12:16.738117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.740142Z","iopub.execute_input":"2025-10-24T16:12:16.740391Z","iopub.status.idle":"2025-10-24T16:12:16.761374Z","shell.execute_reply.started":"2025-10-24T16:12:16.740372Z","shell.execute_reply":"2025-10-24T16:12:16.760360Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### handle ordinal value.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom typing import Dict, List\n\ndef apply_complete_ordinal_encoding(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Applies Custom Ordinal Encoding to all identified quality/rank columns \n    to convert them into numerical values that preserve their inherent order.\n    \n    Assumption: All missing values (NA) in these columns have already been imputed \n    with the string 'None'.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        \n    Returns:\n        pd.DataFrame: The DataFrame with all Ordinal columns successfully encoded to integers.\n    \"\"\"\n    \n    df_encoded = df.copy()\n\n    # 1. MAPPING DEFINITIONS (Defining the numerical rank for each category)\n    \n    # Quality/Condition Scale (6 levels: None/Poorest -> Excellent)\n    # Used for general quality ratings (e.g., ExterQual, BsmtQual)\n    qual_cond_mapping_6 = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    \n    # LotShape (Irregular -> Regular)\n    lot_shape_mapping = {'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4, 'None': 0}\n    \n    # Utilities (Worst -> Best)\n    utilities_mapping = {'ELO': 1, 'NoSeWa': 2, 'NoSewr': 3, 'AllPub': 4, 'None': 0}\n    \n    # LandSlope (Severe -> Gentle)\n    land_slope_mapping = {'Sev': 1, 'Mod': 2, 'Gtl': 3, 'None': 0}\n    \n    # BsmtExposure (No Exposure -> Good Exposure)\n    bsmt_exposure_mapping = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n\n    # BsmtFinType (Unfinished -> Good Living Quarters)\n    bsmt_fin_mapping_7 = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n    \n    # GarageFinish (Unfinished -> Finished)\n    garage_finish_mapping = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n    \n    # Functional Rating (Salvage -> Typical)\n    functional_mapping = {\n        'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, \n        'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8\n    }\n    \n    # Fence Quality (No Fence -> Good Privacy)\n    fence_mapping = {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n\n\n    # ----------------------------------------------------------------------\n    # 2. COLUMN APPLICATION LIST (Complete 19 Ordinal Features)\n    # ----------------------------------------------------------------------\n\n    column_mappings: List[tuple[str, Dict[str, int]]] = [\n        # Quality/Condition - 6-Point Scale\n        ('ExterQual', qual_cond_mapping_6), ('ExterCond', qual_cond_mapping_6),\n        ('HeatingQC', qual_cond_mapping_6), ('KitchenQual', qual_cond_mapping_6),\n        ('BsmtQual', qual_cond_mapping_6), ('BsmtCond', qual_cond_mapping_6),\n        ('FireplaceQu', qual_cond_mapping_6), ('PoolQC', qual_cond_mapping_6),\n        ('GarageQual', qual_cond_mapping_6), ('GarageCond', qual_cond_mapping_6),\n        \n        # Specific Mappings\n        ('LotShape', lot_shape_mapping),\n        ('Utilities', utilities_mapping),\n        ('LandSlope', land_slope_mapping),\n        ('BsmtExposure', bsmt_exposure_mapping),\n        ('BsmtFinType1', bsmt_fin_mapping_7),\n        ('BsmtFinType2', bsmt_fin_mapping_7),\n        ('GarageFinish', garage_finish_mapping),\n        ('Functional', functional_mapping),\n        \n        # Added Fence\n        ('Fence', fence_mapping),\n    ]\n    \n    print(\"--- STARTING CUSTOM ORDINAL ENCODING ---\")\n    \n    for col, mapping in column_mappings:\n        if col in df_encoded.columns:\n            # Apply the mapping. .fillna(0) handles any residual unmapped values.\n            # Using .astype(int) ensures the result is a clean integer type.\n            df_encoded[col] = df_encoded[col].map(mapping).fillna(0).astype(int)\n            print(f\"‚úÖ Encoded: {col}\")\n        else:\n             print(f\"‚ö†Ô∏è Column '{col}' not found. Skipping.\")\n\n    # 3. OverallQual and OverallCond are already numerical (int) and are left alone.\n    print(\"--- ORDINAL ENCODING COMPLETE ---\")\n    \n    return df_encoded\n\n# --- EXAMPLE USAGE ---\ntrain_df_ordinal_encoded = apply_complete_ordinal_encoding(train_df_imputed)\ntest_df_ordinal_encoded = apply_complete_ordinal_encoding(test_df_imputed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.762785Z","iopub.execute_input":"2025-10-24T16:12:16.763036Z","iopub.status.idle":"2025-10-24T16:12:16.827580Z","shell.execute_reply.started":"2025-10-24T16:12:16.763013Z","shell.execute_reply":"2025-10-24T16:12:16.826397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_ordinal_encoded.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.828844Z","iopub.execute_input":"2025-10-24T16:12:16.829468Z","iopub.status.idle":"2025-10-24T16:12:16.850674Z","shell.execute_reply.started":"2025-10-24T16:12:16.829437Z","shell.execute_reply":"2025-10-24T16:12:16.849591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef apply_nominal_label_encoding_after_ordinal_filtered(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Applies Label Encoding to nominal categorical variables, excluding columns \n    that have already been Ordinal Encoded.\n\n    Args:\n        df (pd.DataFrame): The DataFrame after Ordinal Encoding (contains numerical \n                           and remaining 'object' columns).\n    \n    Returns:\n        pd.DataFrame: The DataFrame containing only numerical data \n                      (Label Encoded & ready for scaling/training).\n    \"\"\"\n\n    df_encoded = df.copy()\n\n    # 1Ô∏è‚É£ Ordinal columns that were ALREADY encoded to numbers (these are skipped)\n    # We do this check implicitly by only selecting 'object' columns.\n    \n    # 2Ô∏è‚É£ Select the remaining Nominal columns (type 'object' or 'category')\n    # These are the columns that did not have an inherent order.\n    nominal_cols_to_label = [\n        col for col in df_encoded.select_dtypes(include=['object']).columns\n        # Note: We rely on the fact that Ordinal columns have already been converted to int/float.\n    ]\n\n    if not nominal_cols_to_label:\n        print(\"‚ö†Ô∏è No Nominal columns found that require Label Encoding.\")\n        return df_encoded\n\n    print(\"üîß --- STARTING LABEL ENCODING (NOMINAL FEATURES) ---\")\n    print(f\"üìä Columns to be encoded: {nominal_cols_to_label}\")\n\n    # 3Ô∏è‚É£ Apply LabelEncoder\n    le = LabelEncoder()\n    for col in nominal_cols_to_label:\n        # Convert to string explicitly to handle mixed types safely before encoding\n        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n\n    print(\"‚úÖ --- LABEL ENCODING COMPLETE ---\")\n    print(f\"üìà Total columns after encoding: {df_encoded.shape[1]}\")\n    print(\"üéØ DataFrame now contains only numerical data, ready for Scaling/Training.\")\n\n    return df_encoded\n\n\n# üîπ Example Usage:\ntrain_df_final_encoded = apply_nominal_label_encoding_after_ordinal_filtered(train_df_ordinal_encoded)\ntest_df_final_encoded = apply_nominal_label_encoding_after_ordinal_filtered(test_df_ordinal_encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.851976Z","iopub.execute_input":"2025-10-24T16:12:16.852271Z","iopub.status.idle":"2025-10-24T16:12:16.928063Z","shell.execute_reply.started":"2025-10-24T16:12:16.852244Z","shell.execute_reply":"2025-10-24T16:12:16.926864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_final_encoded.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.929301Z","iopub.execute_input":"2025-10-24T16:12:16.929662Z","iopub.status.idle":"2025-10-24T16:12:16.947657Z","shell.execute_reply.started":"2025-10-24T16:12:16.929634Z","shell.execute_reply":"2025-10-24T16:12:16.946295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler \nimport pandas as pd\nimport numpy as np\n\ndef apply_final_scaling(df_final_encoded: pd.DataFrame, target_col: str = 'SalePrice') -> pd.DataFrame:\n    \n    \n    df_scaled = df_final_encoded.copy()\n    \n    numeric_cols = df_scaled.select_dtypes(include=np.number).columns\n    \n    cols_to_check = numeric_cols.drop(target_col, errors='ignore').tolist()\n    \n    final_cols_to_scale = [col for col in cols_to_check if df_scaled[col].nunique() > 2]\n    \n    scaler = MinMaxScaler()\n    \n    df_scaled[final_cols_to_scale] = scaler.fit_transform(df_scaled[final_cols_to_scale])\n    \n    print(\"--- SCALING Done ---\")\n    print(\"‚úÖ Use MinMaxScaler.\")\n    print(f\"‚úÖ Scaling has been applied to {len(final_cols_to_scale)} column (Ordinal v√† Continuous).\")\n    \n    return df_scaled\n\ntrain_df_final_scaled = apply_final_scaling(train_df_final_encoded, target_col='SalePrice')\ntest_df_final_scaled = apply_final_scaling(test_df_final_encoded, target_col='None')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:16.949008Z","iopub.execute_input":"2025-10-24T16:12:16.949279Z","iopub.status.idle":"2025-10-24T16:12:17.023397Z","shell.execute_reply.started":"2025-10-24T16:12:16.949259Z","shell.execute_reply":"2025-10-24T16:12:17.022138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_final_scaled['SalePrice']= np.log1p(train_df_final_scaled['SalePrice'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:17.024463Z","iopub.execute_input":"2025-10-24T16:12:17.024776Z","iopub.status.idle":"2025-10-24T16:12:17.030324Z","shell.execute_reply.started":"2025-10-24T16:12:17.024744Z","shell.execute_reply":"2025-10-24T16:12:17.029152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Split train and validation data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom typing import Tuple\ndef split_train_validation(df_scaled: pd.DataFrame, target_col: str = 'SalePrice', test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n    X = df_scaled.drop(columns=[target_col])\n    y = df_scaled[target_col]\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    print(\"--- DATA SET SPLIT COMPLETE ---\")\n    print(f\"Shape train data (X_train): {X_train.shape}\")\n    print(f\"Shape validation data(X_val): {X_val.shape}\")\n    print(f\"split ratio: {1 - test_size}:{test_size} (Training: Assessment)\")\n    \n    return X_train, X_val, y_train, y_val\nX_train, X_val, y_train, y_val = split_train_validation(train_df_final_scaled, test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:17.031649Z","iopub.execute_input":"2025-10-24T16:12:17.032031Z","iopub.status.idle":"2025-10-24T16:12:17.066255Z","shell.execute_reply.started":"2025-10-24T16:12:17.031998Z","shell.execute_reply":"2025-10-24T16:12:17.064917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train and evaluate model","metadata":{}},{"cell_type":"code","source":"\n# ==========================================\n# ‚öôÔ∏è 1Ô∏è‚É£ MODEL CONFIGURATION\n# ==========================================\nlinear_params = {\n    'ElasticNet': {\n        'model': ElasticNet(max_iter=5000, random_state=42),\n        'param_grid': {'alpha': [0.0001, 0.0005, 0.001], 'l1_ratio': [0.5, 0.7, 0.9]}\n    },\n    \"Lasso_Reg\": {\n        'model': Lasso(max_iter=5000, random_state=42),\n        'param_grid': {'alpha': [0.0003, 0.0005, 0.0008, 0.001]}\n    }\n}\n\ntree_models = {\n    \"RandomForest\": RandomForestRegressor(n_estimators=1000, max_depth=8, random_state=42, n_jobs=-1),\n    \"XGBoost\": XGBRegressor(n_estimators=2000, learning_rate=0.01, max_depth=3, random_state=42, n_jobs=-1)\n}\n\nresults = {}\n\n# ==========================================\n# üßÆ 2Ô∏è‚É£ TRAINING ON LOG(SALEPRICE)\n# ==========================================\nprint(\"\\n--- üîÅ MODEL TRAINING & HYPERPARAMETER TUNING ---\")\n\nfor name, config in linear_params.items():\n    start_time = time.time()\n    print(f\"\\n‚öôÔ∏è GridSearchCV for {name} ...\")\n\n    grid_search = GridSearchCV(\n        config['model'],\n        config['param_grid'],\n        cv=5,\n        scoring='neg_mean_squared_error',\n        n_jobs=-1\n    )\n    grid_search.fit(X_train, y_train)  # y_train is already log(SalePrice)\n    best_model = grid_search.best_estimator_\n\n    # --- PREDICTION ---\n    y_pred_val_log = best_model.predict(X_val)\n\n    # --- EVALUATION ---\n    rmse_log = np.sqrt(mean_squared_error(y_val, y_pred_val_log))\n    r2_val = r2_score(y_val, y_pred_val_log)\n\n    # Inverse transform to original currency unit\n    y_val_true = np.expm1(y_val)\n    y_val_pred = np.expm1(y_pred_val_log)\n    rmse_real = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n\n    results[name] = {\n        \"RMSE_Log\": round(rmse_log, 4),\n        \"R2_Score\": round(r2_val, 4),\n        \"RMSE_Original($)\": round(rmse_real, 2),\n        \"Best_Alpha\": grid_search.best_params_.get('alpha'),\n        \"Best_L1_Ratio\": grid_search.best_params_.get('l1_ratio', 'N/A'),\n        \"Model\": best_model\n    }\n    print(f\"‚úÖ {name} complete. | RMSE_Log: {rmse_log:.4f} | Time: {time.time() - start_time:.2f}s\")\n\n# ---------------------------------------------------\n# üå≥ 3Ô∏è‚É£ TREE-BASED MODELS\n# ---------------------------------------------------\nfor name, model in tree_models.items():\n    start_time = time.time()\n    print(f\"\\nüå≤ Training {name} ...\")\n\n    model.fit(X_train, y_train)\n    y_pred_val_log = model.predict(X_val)\n\n    rmse_log = np.sqrt(mean_squared_error(y_val, y_pred_val_log))\n    r2_val = r2_score(y_val, y_pred_val_log)\n\n    y_val_true = np.expm1(y_val)\n    y_val_pred = np.expm1(y_pred_val_log)\n    rmse_real = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n\n    results[name] = {\n        \"RMSE_Log\": round(rmse_log, 4),\n        \"R2_Score\": round(r2_val, 4),\n        \"RMSE_Original($)\": round(rmse_real, 2),\n        \"Model\": model\n    }\n    print(f\"‚úÖ {name} complete. | RMSE_Log: {rmse_log:.4f} | Time: {time.time() - start_time:.2f}s\")\n\n# ---------------------------------------------------\n# üìä 4Ô∏è‚É£ BEST MODEL SELECTION\n# ---------------------------------------------------\nresults_df = pd.DataFrame(results).T.sort_values(by=\"RMSE_Log\")\nprint(\"\\n--- üßæ PERFORMANCE ON VALIDATION ---\")\nprint(results_df.drop(columns=['Model']))\n\nbest_model_name = results_df.index[0]\nfinal_model = results_df.loc[best_model_name, 'Model']\n\n# ---------------------------------------------------\n# üß† 5Ô∏è‚É£ RETRAIN ON FULL TRAIN SET\n# ---------------------------------------------------\nX_full = pd.concat([X_train, X_val], axis=0, ignore_index=True)\ny_full = pd.concat([y_train, y_val], axis=0, ignore_index=True)  # Still log(SalePrice)\n\nfinal_model.fit(X_full, y_full)\nprint(f\"\\n‚úÖ Retrained best model ({best_model_name}) on full train data.\")\n\n# ---------------------------------------------------\n# üß© 6Ô∏è‚É£ PREPARE FINAL TEST SET (Assuming necessary features are aligned in test_df)\n# ---------------------------------------------------\n# Note: The original script assumes a merged 'test_full_df' exists for the prediction and submission step.\n# We will use the provided test DataFrame structure.\n# X_test_final is assumed to be the FINAL, PROCESSED, SCALED test features.\nX_test_final = test_df_final_scaled[X_full.columns]\n\n# ---------------------------------------------------\n# üîÆ 7Ô∏è‚É£ PREDICTION & INVERSE TRANSFORM\n# ---------------------------------------------------\ny_test_pred_log = final_model.predict(X_test_final)\ny_test_pred_real = np.expm1(y_test_pred_log)\n\n# Assuming test_df (the raw test data, needed for ID and potential SalePrice lookup) \n# is merged with a sample submission or similar structure.\n# We create a final prediction dataframe.\n# Note: The original script uses a structure based on merging with a 'sample_df' which is not defined here.\ntest_full_df['Predicted_SalePrice'] = y_test_pred_real\n\n# ---------------------------------------------------\n# üìà 8Ô∏è‚É£ EVALUATION ON TEST (IF TRUE PRICES EXIST)\n# ---------------------------------------------------\nif 'SalePrice' in test_full_df.columns and test_full_df['SalePrice'].notna().sum() > 0:\n    y_true = test_full_df['SalePrice']\n    y_pred = test_full_df['Predicted_SalePrice']\n\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n    print(\"\\n--- üìä PERFORMANCE ON TEST ---\")\n    print(f\"RMSE : {rmse:,.2f}\")\n    print(f\"MAE  : {mae:,.2f}\")\n    print(f\"MAPE : {mape:.2f}%\")\nelse:\n    print(\"\\n‚ö†Ô∏è Evaluation skipped: True SalePrice is not available in the test set.\")\n\n# ---------------------------------------------------\n# üíæ 9Ô∏è‚É£ SUBMISSION FILE\n# ---------------------------------------------------\nsubmission = test_full_df[['Id', 'Predicted_SalePrice']].rename(columns={'Predicted_SalePrice': 'SalePrice'})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"\\n‚úÖ Saved submission.csv\")\n\n# ---------------------------------------------------\n# üîç üîü SAMPLE RESULTS\n# ---------------------------------------------------\nprint(\"\\n--- SAMPLE PREDICTIONS ---\")\nprint(test_full_df[['Id', 'SalePrice', 'Predicted_SalePrice']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:17.067344Z","iopub.execute_input":"2025-10-24T16:12:17.067661Z","iopub.status.idle":"2025-10-24T16:12:28.091034Z","shell.execute_reply.started":"2025-10-24T16:12:17.067638Z","shell.execute_reply":"2025-10-24T16:12:28.089755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplt.figure(figsize=(6,6))\nplt.scatter(y_true, y_pred, alpha=0.6)\nplt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\nplt.xlabel(\"Actual price\")\nplt.ylabel(\"Predict price\")\nplt.title(\"üìä Compare real SalePrice vs prediction (Test set)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:12:28.093051Z","iopub.execute_input":"2025-10-24T16:12:28.093321Z","iopub.status.idle":"2025-10-24T16:12:28.315614Z","shell.execute_reply.started":"2025-10-24T16:12:28.093301Z","shell.execute_reply":"2025-10-24T16:12:28.314182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}